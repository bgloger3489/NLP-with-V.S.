{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Reviews: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import os.path\n",
    "import glob\n",
    "import random\n",
    "\n",
    "#Dataset: https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset\n",
    "\n",
    "filenames = glob.glob(os.path.join(os.getcwd(),\"/Users/Ben/Desktop/Vital Strategies/Datasets/Imdb/train/neg/*.txt\"))\n",
    "posFilenames = glob.glob(os.path.join(os.getcwd(),\"/Users/Ben/Desktop/Vital Strategies/Datasets/Imdb/train/pos/*.txt\"))\n",
    "\n",
    "for p in posFilenames:\n",
    "    filenames.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "textcat = nlp.create_pipe(\n",
    "            \"textcat\",\n",
    "            config={\n",
    "                \"exclusive_classes\": True,\n",
    "                \"architecture\": \"simple_cnn\",\n",
    "            }\n",
    "        )\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 / 2\n",
      "  batch:  0 / 250\n",
      "  batch:  1 / 250\n",
      "  batch:  2 / 250\n",
      "  batch:  3 / 250\n",
      "  batch:  4 / 250\n",
      "  batch:  5 / 250\n",
      "  batch:  6 / 250\n",
      "  batch:  7 / 250\n",
      "  batch:  8 / 250\n",
      "  batch:  9 / 250\n",
      "  batch:  10 / 250\n",
      "  batch:  11 / 250\n",
      "  batch:  12 / 250\n",
      "  batch:  13 / 250\n",
      "  batch:  14 / 250\n",
      "  batch:  15 / 250\n",
      "  batch:  16 / 250\n",
      "  batch:  17 / 250\n",
      "  batch:  18 / 250\n",
      "  batch:  19 / 250\n",
      "  batch:  20 / 250\n",
      "  batch:  21 / 250\n",
      "  batch:  22 / 250\n",
      "  batch:  23 / 250\n",
      "  batch:  24 / 250\n",
      "  batch:  25 / 250\n",
      "  batch:  26 / 250\n",
      "  batch:  27 / 250\n",
      "  batch:  28 / 250\n",
      "  batch:  29 / 250\n",
      "  batch:  30 / 250\n",
      "  batch:  31 / 250\n",
      "  batch:  32 / 250\n",
      "  batch:  33 / 250\n",
      "  batch:  34 / 250\n",
      "  batch:  35 / 250\n",
      "  batch:  36 / 250\n",
      "  batch:  37 / 250\n",
      "  batch:  38 / 250\n",
      "  batch:  39 / 250\n",
      "  batch:  40 / 250\n",
      "  batch:  41 / 250\n",
      "  batch:  42 / 250\n",
      "  batch:  43 / 250\n",
      "  batch:  44 / 250\n",
      "  batch:  45 / 250\n",
      "  batch:  46 / 250\n",
      "  batch:  47 / 250\n",
      "  batch:  48 / 250\n",
      "  batch:  49 / 250\n",
      "  batch:  50 / 250\n",
      "  batch:  51 / 250\n",
      "  batch:  52 / 250\n",
      "  batch:  53 / 250\n",
      "  batch:  54 / 250\n",
      "  batch:  55 / 250\n",
      "  batch:  56 / 250\n",
      "  batch:  57 / 250\n",
      "  batch:  58 / 250\n",
      "  batch:  59 / 250\n",
      "  batch:  60 / 250\n",
      "  batch:  61 / 250\n",
      "  batch:  62 / 250\n",
      "  batch:  63 / 250\n",
      "  batch:  64 / 250\n",
      "  batch:  65 / 250\n",
      "  batch:  66 / 250\n",
      "  batch:  67 / 250\n",
      "  batch:  68 / 250\n",
      "  batch:  69 / 250\n",
      "  batch:  70 / 250\n",
      "  batch:  71 / 250\n",
      "  batch:  72 / 250\n",
      "  batch:  73 / 250\n",
      "  batch:  74 / 250\n",
      "  batch:  75 / 250\n",
      "  batch:  76 / 250\n",
      "  batch:  77 / 250\n",
      "  batch:  78 / 250\n",
      "  batch:  79 / 250\n",
      "  batch:  80 / 250\n",
      "  batch:  81 / 250\n",
      "  batch:  82 / 250\n",
      "  batch:  83 / 250\n",
      "  batch:  84 / 250\n",
      "  batch:  85 / 250\n",
      "  batch:  86 / 250\n",
      "  batch:  87 / 250\n",
      "  batch:  88 / 250\n",
      "  batch:  89 / 250\n",
      "  batch:  90 / 250\n",
      "  batch:  91 / 250\n",
      "  batch:  92 / 250\n",
      "  batch:  93 / 250\n",
      "  batch:  94 / 250\n",
      "  batch:  95 / 250\n",
      "  batch:  96 / 250\n",
      "  batch:  97 / 250\n",
      "  batch:  98 / 250\n",
      "  batch:  99 / 250\n",
      "  batch:  100 / 250\n",
      "  batch:  101 / 250\n",
      "  batch:  102 / 250\n",
      "  batch:  103 / 250\n",
      "  batch:  104 / 250\n",
      "  batch:  105 / 250\n",
      "  batch:  106 / 250\n",
      "  batch:  107 / 250\n",
      "  batch:  108 / 250\n",
      "  batch:  109 / 250\n",
      "  batch:  110 / 250\n",
      "  batch:  111 / 250\n",
      "  batch:  112 / 250\n",
      "  batch:  113 / 250\n",
      "  batch:  114 / 250\n",
      "  batch:  115 / 250\n",
      "  batch:  116 / 250\n",
      "  batch:  117 / 250\n",
      "  batch:  118 / 250\n",
      "  batch:  119 / 250\n",
      "  batch:  120 / 250\n",
      "  batch:  121 / 250\n",
      "  batch:  122 / 250\n",
      "  batch:  123 / 250\n",
      "  batch:  124 / 250\n",
      "  batch:  125 / 250\n",
      "  batch:  126 / 250\n",
      "  batch:  127 / 250\n",
      "  batch:  128 / 250\n",
      "  batch:  129 / 250\n",
      "  batch:  130 / 250\n",
      "  batch:  131 / 250\n",
      "  batch:  132 / 250\n",
      "  batch:  133 / 250\n",
      "  batch:  134 / 250\n",
      "  batch:  135 / 250\n",
      "  batch:  136 / 250\n",
      "  batch:  137 / 250\n",
      "  batch:  138 / 250\n",
      "  batch:  139 / 250\n",
      "  batch:  140 / 250\n",
      "  batch:  141 / 250\n",
      "  batch:  142 / 250\n",
      "  batch:  143 / 250\n",
      "  batch:  144 / 250\n",
      "  batch:  145 / 250\n",
      "  batch:  146 / 250\n",
      "  batch:  147 / 250\n",
      "  batch:  148 / 250\n",
      "  batch:  149 / 250\n",
      "  batch:  150 / 250\n",
      "  batch:  151 / 250\n",
      "  batch:  152 / 250\n",
      "  batch:  153 / 250\n",
      "  batch:  154 / 250\n",
      "  batch:  155 / 250\n",
      "  batch:  156 / 250\n",
      "  batch:  157 / 250\n",
      "  batch:  158 / 250\n",
      "  batch:  159 / 250\n",
      "  batch:  160 / 250\n",
      "  batch:  161 / 250\n",
      "  batch:  162 / 250\n",
      "  batch:  163 / 250\n",
      "  batch:  164 / 250\n",
      "  batch:  165 / 250\n",
      "  batch:  166 / 250\n",
      "  batch:  167 / 250\n",
      "  batch:  168 / 250\n",
      "  batch:  169 / 250\n",
      "  batch:  170 / 250\n",
      "  batch:  171 / 250\n",
      "  batch:  172 / 250\n",
      "  batch:  173 / 250\n",
      "  batch:  174 / 250\n",
      "  batch:  175 / 250\n",
      "  batch:  176 / 250\n",
      "  batch:  177 / 250\n",
      "  batch:  178 / 250\n",
      "  batch:  179 / 250\n",
      "  batch:  180 / 250\n",
      "  batch:  181 / 250\n",
      "  batch:  182 / 250\n",
      "  batch:  183 / 250\n",
      "  batch:  184 / 250\n",
      "  batch:  185 / 250\n",
      "  batch:  186 / 250\n",
      "  batch:  187 / 250\n",
      "  batch:  188 / 250\n",
      "  batch:  189 / 250\n",
      "  batch:  190 / 250\n",
      "  batch:  191 / 250\n",
      "  batch:  192 / 250\n",
      "  batch:  193 / 250\n",
      "  batch:  194 / 250\n",
      "  batch:  195 / 250\n",
      "  batch:  196 / 250\n",
      "  batch:  197 / 250\n",
      "  batch:  198 / 250\n",
      "  batch:  199 / 250\n",
      "  batch:  200 / 250\n",
      "  batch:  201 / 250\n",
      "  batch:  202 / 250\n",
      "  batch:  203 / 250\n",
      "  batch:  204 / 250\n",
      "  batch:  205 / 250\n",
      "  batch:  206 / 250\n",
      "  batch:  207 / 250\n",
      "  batch:  208 / 250\n",
      "  batch:  209 / 250\n",
      "  batch:  210 / 250\n",
      "  batch:  211 / 250\n",
      "  batch:  212 / 250\n",
      "  batch:  213 / 250\n",
      "  batch:  214 / 250\n",
      "  batch:  215 / 250\n",
      "  batch:  216 / 250\n",
      "  batch:  217 / 250\n",
      "  batch:  218 / 250\n",
      "  batch:  219 / 250\n",
      "  batch:  220 / 250\n",
      "  batch:  221 / 250\n",
      "  batch:  222 / 250\n",
      "  batch:  223 / 250\n",
      "  batch:  224 / 250\n",
      "  batch:  225 / 250\n",
      "  batch:  226 / 250\n",
      "  batch:  227 / 250\n",
      "  batch:  228 / 250\n",
      "  batch:  229 / 250\n",
      "  batch:  230 / 250\n",
      "  batch:  231 / 250\n",
      "  batch:  232 / 250\n",
      "  batch:  233 / 250\n",
      "  batch:  234 / 250\n",
      "  batch:  235 / 250\n",
      "  batch:  236 / 250\n",
      "  batch:  237 / 250\n",
      "  batch:  238 / 250\n",
      "  batch:  239 / 250\n",
      "  batch:  240 / 250\n",
      "  batch:  241 / 250\n",
      "  batch:  242 / 250\n",
      "  batch:  243 / 250\n",
      "  batch:  244 / 250\n",
      "  batch:  245 / 250\n",
      "  batch:  246 / 250\n",
      "  batch:  247 / 250\n",
      "  batch:  248 / 250\n",
      "  batch:  249 / 250\n",
      "epoch:  1 / 2\n",
      "  batch:  0 / 250\n",
      "  batch:  1 / 250\n",
      "  batch:  2 / 250\n",
      "  batch:  3 / 250\n",
      "  batch:  4 / 250\n",
      "  batch:  5 / 250\n",
      "  batch:  6 / 250\n",
      "  batch:  7 / 250\n",
      "  batch:  8 / 250\n",
      "  batch:  9 / 250\n",
      "  batch:  10 / 250\n",
      "  batch:  11 / 250\n",
      "  batch:  12 / 250\n",
      "  batch:  13 / 250\n",
      "  batch:  14 / 250\n",
      "  batch:  15 / 250\n",
      "  batch:  16 / 250\n",
      "  batch:  17 / 250\n",
      "  batch:  18 / 250\n",
      "  batch:  19 / 250\n",
      "  batch:  20 / 250\n",
      "  batch:  21 / 250\n",
      "  batch:  22 / 250\n",
      "  batch:  23 / 250\n",
      "  batch:  24 / 250\n",
      "  batch:  25 / 250\n",
      "  batch:  26 / 250\n",
      "  batch:  27 / 250\n",
      "  batch:  28 / 250\n",
      "  batch:  29 / 250\n",
      "  batch:  30 / 250\n",
      "  batch:  31 / 250\n",
      "  batch:  32 / 250\n",
      "  batch:  33 / 250\n",
      "  batch:  34 / 250\n",
      "  batch:  35 / 250\n",
      "  batch:  36 / 250\n",
      "  batch:  37 / 250\n",
      "  batch:  38 / 250\n",
      "  batch:  39 / 250\n",
      "  batch:  40 / 250\n",
      "  batch:  41 / 250\n",
      "  batch:  42 / 250\n",
      "  batch:  43 / 250\n",
      "  batch:  44 / 250\n",
      "  batch:  45 / 250\n",
      "  batch:  46 / 250\n",
      "  batch:  47 / 250\n",
      "  batch:  48 / 250\n",
      "  batch:  49 / 250\n",
      "  batch:  50 / 250\n",
      "  batch:  51 / 250\n",
      "  batch:  52 / 250\n",
      "  batch:  53 / 250\n",
      "  batch:  54 / 250\n",
      "  batch:  55 / 250\n",
      "  batch:  56 / 250\n",
      "  batch:  57 / 250\n",
      "  batch:  58 / 250\n",
      "  batch:  59 / 250\n",
      "  batch:  60 / 250\n",
      "  batch:  61 / 250\n",
      "  batch:  62 / 250\n",
      "  batch:  63 / 250\n",
      "  batch:  64 / 250\n",
      "  batch:  65 / 250\n",
      "  batch:  66 / 250\n",
      "  batch:  67 / 250\n",
      "  batch:  68 / 250\n",
      "  batch:  69 / 250\n",
      "  batch:  70 / 250\n",
      "  batch:  71 / 250\n",
      "  batch:  72 / 250\n",
      "  batch:  73 / 250\n",
      "  batch:  74 / 250\n",
      "  batch:  75 / 250\n",
      "  batch:  76 / 250\n",
      "  batch:  77 / 250\n",
      "  batch:  78 / 250\n",
      "  batch:  79 / 250\n",
      "  batch:  80 / 250\n",
      "  batch:  81 / 250\n",
      "  batch:  82 / 250\n",
      "  batch:  83 / 250\n",
      "  batch:  84 / 250\n",
      "  batch:  85 / 250\n",
      "  batch:  86 / 250\n",
      "  batch:  87 / 250\n",
      "  batch:  88 / 250\n",
      "  batch:  89 / 250\n",
      "  batch:  90 / 250\n",
      "  batch:  91 / 250\n",
      "  batch:  92 / 250\n",
      "  batch:  93 / 250\n",
      "  batch:  94 / 250\n",
      "  batch:  95 / 250\n",
      "  batch:  96 / 250\n",
      "  batch:  97 / 250\n",
      "  batch:  98 / 250\n",
      "  batch:  99 / 250\n",
      "  batch:  100 / 250\n",
      "  batch:  101 / 250\n",
      "  batch:  102 / 250\n",
      "  batch:  103 / 250\n",
      "  batch:  104 / 250\n",
      "  batch:  105 / 250\n",
      "  batch:  106 / 250\n",
      "  batch:  107 / 250\n",
      "  batch:  108 / 250\n",
      "  batch:  109 / 250\n",
      "  batch:  110 / 250\n",
      "  batch:  111 / 250\n",
      "  batch:  112 / 250\n",
      "  batch:  113 / 250\n",
      "  batch:  114 / 250\n",
      "  batch:  115 / 250\n",
      "  batch:  116 / 250\n",
      "  batch:  117 / 250\n",
      "  batch:  118 / 250\n",
      "  batch:  119 / 250\n",
      "  batch:  120 / 250\n",
      "  batch:  121 / 250\n",
      "  batch:  122 / 250\n",
      "  batch:  123 / 250\n",
      "  batch:  124 / 250\n",
      "  batch:  125 / 250\n",
      "  batch:  126 / 250\n",
      "  batch:  127 / 250\n",
      "  batch:  128 / 250\n",
      "  batch:  129 / 250\n",
      "  batch:  130 / 250\n",
      "  batch:  131 / 250\n",
      "  batch:  132 / 250\n",
      "  batch:  133 / 250\n",
      "  batch:  134 / 250\n",
      "  batch:  135 / 250\n",
      "  batch:  136 / 250\n",
      "  batch:  137 / 250\n",
      "  batch:  138 / 250\n",
      "  batch:  139 / 250\n",
      "  batch:  140 / 250\n",
      "  batch:  141 / 250\n",
      "  batch:  142 / 250\n",
      "  batch:  143 / 250\n",
      "  batch:  144 / 250\n",
      "  batch:  145 / 250\n",
      "  batch:  146 / 250\n",
      "  batch:  147 / 250\n",
      "  batch:  148 / 250\n",
      "  batch:  149 / 250\n",
      "  batch:  150 / 250\n",
      "  batch:  151 / 250\n",
      "  batch:  152 / 250\n",
      "  batch:  153 / 250\n",
      "  batch:  154 / 250\n",
      "  batch:  155 / 250\n",
      "  batch:  156 / 250\n",
      "  batch:  157 / 250\n",
      "  batch:  158 / 250\n",
      "  batch:  159 / 250\n",
      "  batch:  160 / 250\n",
      "  batch:  161 / 250\n",
      "  batch:  162 / 250\n",
      "  batch:  163 / 250\n",
      "  batch:  164 / 250\n",
      "  batch:  165 / 250\n",
      "  batch:  166 / 250\n",
      "  batch:  167 / 250\n",
      "  batch:  168 / 250\n",
      "  batch:  169 / 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch:  170 / 250\n",
      "  batch:  171 / 250\n",
      "  batch:  172 / 250\n",
      "  batch:  173 / 250\n",
      "  batch:  174 / 250\n",
      "  batch:  175 / 250\n",
      "  batch:  176 / 250\n",
      "  batch:  177 / 250\n",
      "  batch:  178 / 250\n",
      "  batch:  179 / 250\n",
      "  batch:  180 / 250\n",
      "  batch:  181 / 250\n",
      "  batch:  182 / 250\n",
      "  batch:  183 / 250\n",
      "  batch:  184 / 250\n",
      "  batch:  185 / 250\n",
      "  batch:  186 / 250\n",
      "  batch:  187 / 250\n",
      "  batch:  188 / 250\n",
      "  batch:  189 / 250\n",
      "  batch:  190 / 250\n",
      "  batch:  191 / 250\n",
      "  batch:  192 / 250\n",
      "  batch:  193 / 250\n",
      "  batch:  194 / 250\n",
      "  batch:  195 / 250\n",
      "  batch:  196 / 250\n",
      "  batch:  197 / 250\n",
      "  batch:  198 / 250\n",
      "  batch:  199 / 250\n",
      "  batch:  200 / 250\n",
      "  batch:  201 / 250\n",
      "  batch:  202 / 250\n",
      "  batch:  203 / 250\n",
      "  batch:  204 / 250\n",
      "  batch:  205 / 250\n",
      "  batch:  206 / 250\n",
      "  batch:  207 / 250\n",
      "  batch:  208 / 250\n",
      "  batch:  209 / 250\n",
      "  batch:  210 / 250\n",
      "  batch:  211 / 250\n",
      "  batch:  212 / 250\n",
      "  batch:  213 / 250\n",
      "  batch:  214 / 250\n",
      "  batch:  215 / 250\n",
      "  batch:  216 / 250\n",
      "  batch:  217 / 250\n",
      "  batch:  218 / 250\n",
      "  batch:  219 / 250\n",
      "  batch:  220 / 250\n",
      "  batch:  221 / 250\n",
      "  batch:  222 / 250\n",
      "  batch:  223 / 250\n",
      "  batch:  224 / 250\n",
      "  batch:  225 / 250\n",
      "  batch:  226 / 250\n",
      "  batch:  227 / 250\n",
      "  batch:  228 / 250\n",
      "  batch:  229 / 250\n",
      "  batch:  230 / 250\n",
      "  batch:  231 / 250\n",
      "  batch:  232 / 250\n",
      "  batch:  233 / 250\n",
      "  batch:  234 / 250\n",
      "  batch:  235 / 250\n",
      "  batch:  236 / 250\n",
      "  batch:  237 / 250\n",
      "  batch:  238 / 250\n",
      "  batch:  239 / 250\n",
      "  batch:  240 / 250\n",
      "  batch:  241 / 250\n",
      "  batch:  242 / 250\n",
      "  batch:  243 / 250\n",
      "  batch:  244 / 250\n",
      "  batch:  245 / 250\n",
      "  batch:  246 / 250\n",
      "  batch:  247 / 250\n",
      "  batch:  248 / 250\n",
      "  batch:  249 / 250\n"
     ]
    }
   ],
   "source": [
    "#FORMAT: train_data = [(u\"Uber blew through $1 million\", {\"entities\": [(0, 4, \"ORG\")]})]\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    \n",
    "    len_of_data = len(filenames)\n",
    "    \n",
    "    idxs = list(range(len_of_data))\n",
    "    \n",
    "    ITERATIONS = 2\n",
    "    for i in range(ITERATIONS):\n",
    "        print(\"epoch: \", i, \"/\", ITERATIONS)\n",
    "        random.shuffle(idxs)\n",
    "        \n",
    "        \n",
    "        #Batches:\n",
    "        SIZE = 100\n",
    "        if(len_of_data % SIZE != 0):\n",
    "            print(\"BAD BATCH SIZE!\")\n",
    "        for batch in range(int(len_of_data/SIZE)):\n",
    "            print(\"  batch: \",batch , \"/\", int(len_of_data/SIZE))\n",
    "            annotations = []\n",
    "            texts = []\n",
    "            for idx in idxs[batch*SIZE:SIZE*(batch+1)]:\n",
    "                with open(filenames[idx],\"r\") as f:\n",
    "                    texts.append(f.read())\n",
    "                if(idx < len_of_data/2):\n",
    "                    annotations.append({\"cats\":{\"NEGATIVE\":True, \"POSITIVE\":False}})\n",
    "                else:\n",
    "                    annotations.append({\"cats\":{\"NEGATIVE\":False, \"POSITIVE\":True}})\n",
    "            nlp.update(texts, annotations, sgd=optimizer)\n",
    "    \n",
    "print(\"Training Complete! Saving model...\")\n",
    "nlp.to_disk(os.path.join(os.getcwd(),\"/Users/Ben/Desktop/Vital Strategies/trained models/sentimentClassifier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load((os.path.join(os.getcwd(),\n",
    "                                       \"/Users/Ben/Desktop/Vital Strategies/trained models/sentimentClassifier\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'textcat']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify:\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = glob.glob(os.path.join(os.getcwd(),\n",
    "                                        \"/Users/Ben/Desktop/Vital Strategies/Datasets/Imdb/test/neg/*.txt\"))\n",
    "test_posFilenames = glob.glob(os.path.join(os.getcwd(),\n",
    "                                           \"/Users/Ben/Desktop/Vital Strategies/Datasets/Imdb/test/pos/*.txt\"))\n",
    "\n",
    "\n",
    "for p in test_posFilenames:\n",
    "    test_filenames.append(p)\n",
    "\n",
    "len_of_test = len(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model...\n",
      "idx = 1  Accuracy = 1.0\n",
      "idx = 101  Accuracy = 0.7920792079207921\n",
      "idx = 201  Accuracy = 0.8407960199004975\n",
      "idx = 301  Accuracy = 0.8438538205980066\n",
      "idx = 401  Accuracy = 0.8628428927680798\n",
      "idx = 501  Accuracy = 0.8702594810379242\n",
      "idx = 601  Accuracy = 0.8635607321131448\n",
      "idx = 701  Accuracy = 0.8601997146932953\n",
      "idx = 801  Accuracy = 0.8614232209737828\n",
      "idx = 901  Accuracy = 0.8501664816870145\n",
      "idx = 1001  Accuracy = 0.8491508491508492\n",
      "idx = 1101  Accuracy = 0.8510445049954587\n",
      "idx = 1201  Accuracy = 0.8559533721898418\n",
      "idx = 1301  Accuracy = 0.8562644119907763\n",
      "idx = 1401  Accuracy = 0.8558172733761599\n",
      "idx = 1501  Accuracy = 0.8534310459693538\n",
      "idx = 1601  Accuracy = 0.8519675202998126\n",
      "idx = 1701  Accuracy = 0.84891240446796\n",
      "idx = 1801  Accuracy = 0.8489727928928373\n",
      "idx = 1901  Accuracy = 0.8500789058390321\n",
      "idx = 2001  Accuracy = 0.8525737131434283\n",
      "idx = 2101  Accuracy = 0.8538791051880057\n",
      "idx = 2201  Accuracy = 0.8518855065879146\n",
      "idx = 2301  Accuracy = 0.8518035636679705\n",
      "idx = 2401  Accuracy = 0.8542274052478134\n",
      "idx = 2501  Accuracy = 0.8544582167133147\n",
      "idx = 2601  Accuracy = 0.8539023452518262\n",
      "idx = 2701  Accuracy = 0.853387634209552\n",
      "idx = 2801  Accuracy = 0.855765797929311\n",
      "idx = 2901  Accuracy = 0.8572905894519132\n",
      "idx = 3001  Accuracy = 0.8560479840053316\n",
      "idx = 3101  Accuracy = 0.854240567558852\n",
      "idx = 3201  Accuracy = 0.851921274601687\n",
      "idx = 3301  Accuracy = 0.8524689488033929\n",
      "idx = 3401  Accuracy = 0.8532784475154367\n",
      "idx = 3501  Accuracy = 0.8517566409597258\n",
      "idx = 3601  Accuracy = 0.8525409608442099\n",
      "idx = 3701  Accuracy = 0.8532828965144555\n",
      "idx = 3801  Accuracy = 0.8539857932123125\n",
      "idx = 3901  Accuracy = 0.8541399641117662\n",
      "idx = 4001  Accuracy = 0.8535366158460385\n",
      "idx = 4101  Accuracy = 0.8554011216776396\n",
      "idx = 4201  Accuracy = 0.8550345155915259\n",
      "idx = 4301  Accuracy = 0.8556149732620321\n",
      "idx = 4401  Accuracy = 0.8561690524880708\n",
      "idx = 4501  Accuracy = 0.8566985114419018\n",
      "idx = 4601  Accuracy = 0.8563355792219083\n",
      "idx = 4701  Accuracy = 0.8564135290363752\n",
      "idx = 4801  Accuracy = 0.8554467819204332\n",
      "idx = 4901  Accuracy = 0.8553356457865742\n",
      "idx = 5001  Accuracy = 0.8554289142171566\n",
      "idx = 5101  Accuracy = 0.8543422858263086\n",
      "idx = 5201  Accuracy = 0.8542587963853105\n",
      "idx = 5301  Accuracy = 0.8547443878513488\n",
      "idx = 5401  Accuracy = 0.8544713941862618\n",
      "idx = 5501  Accuracy = 0.8545718960189057\n",
      "idx = 5601  Accuracy = 0.8555615068737725\n",
      "idx = 5701  Accuracy = 0.854937730222768\n",
      "idx = 5801  Accuracy = 0.8553697638338218\n",
      "idx = 5901  Accuracy = 0.8552787663107948\n",
      "idx = 6001  Accuracy = 0.85502416263956\n",
      "idx = 6101  Accuracy = 0.8549418128175709\n",
      "idx = 6201  Accuracy = 0.855668440574101\n",
      "idx = 6301  Accuracy = 0.8557371845738772\n",
      "idx = 6401  Accuracy = 0.8556475550695204\n",
      "idx = 6501  Accuracy = 0.8552530379941548\n",
      "idx = 6601  Accuracy = 0.8562339039539464\n",
      "idx = 6701  Accuracy = 0.8561408744963438\n",
      "idx = 6801  Accuracy = 0.8564916923981767\n",
      "idx = 6901  Accuracy = 0.8568323431386755\n",
      "idx = 7001  Accuracy = 0.8567347521782602\n",
      "idx = 7101  Accuracy = 0.8579073369947895\n",
      "idx = 7201  Accuracy = 0.8577975281210943\n",
      "idx = 7301  Accuracy = 0.8568689220654705\n",
      "idx = 7401  Accuracy = 0.857451695716795\n",
      "idx = 7501  Accuracy = 0.8573523530195973\n",
      "idx = 7601  Accuracy = 0.8568609393500856\n",
      "idx = 7701  Accuracy = 0.8563822880145435\n",
      "idx = 7801  Accuracy = 0.8565568516856813\n",
      "idx = 7901  Accuracy = 0.8564738640678395\n",
      "idx = 8001  Accuracy = 0.856142982127234\n",
      "idx = 8101  Accuracy = 0.8559437106530058\n",
      "idx = 8201  Accuracy = 0.8562370442628948\n",
      "idx = 8301  Accuracy = 0.855800505963137\n",
      "idx = 8401  Accuracy = 0.8550172598500179\n",
      "idx = 8501  Accuracy = 0.8551935066462769\n",
      "idx = 8601  Accuracy = 0.8552493896058598\n",
      "idx = 8701  Accuracy = 0.8553039880473509\n",
      "idx = 8801  Accuracy = 0.855584592659925\n",
      "idx = 8901  Accuracy = 0.8556341984046736\n",
      "idx = 9001  Accuracy = 0.8556827019220087\n",
      "idx = 9101  Accuracy = 0.85649928579277\n",
      "idx = 9201  Accuracy = 0.8566460167373111\n",
      "idx = 9301  Accuracy = 0.856467046554134\n",
      "idx = 9401  Accuracy = 0.8567173704925009\n",
      "idx = 9501  Accuracy = 0.8572781812440796\n",
      "idx = 9601  Accuracy = 0.8574106863868347\n",
      "idx = 9701  Accuracy = 0.8575404597464179\n",
      "idx = 9801  Accuracy = 0.8574635241301908\n",
      "idx = 9901  Accuracy = 0.8576911423088577\n",
      "idx = 10001  Accuracy = 0.8576142385761424\n",
      "idx = 10101  Accuracy = 0.8576378576378576\n",
      "idx = 10201  Accuracy = 0.8577590432310558\n",
      "idx = 10301  Accuracy = 0.8576837200271819\n",
      "idx = 10401  Accuracy = 0.8580905682145947\n",
      "idx = 10501  Accuracy = 0.8579182934958576\n",
      "idx = 10601  Accuracy = 0.8581265918309593\n",
      "idx = 10701  Accuracy = 0.8577703018409495\n",
      "idx = 10801  Accuracy = 0.8578835293028423\n",
      "idx = 10901  Accuracy = 0.8582698834969269\n",
      "idx = 11001  Accuracy = 0.8584674120534497\n",
      "idx = 11101  Accuracy = 0.8578506440861183\n",
      "idx = 11201  Accuracy = 0.8582269440228552\n",
      "idx = 11301  Accuracy = 0.8581541456508274\n",
      "idx = 11401  Accuracy = 0.85843347074818\n",
      "idx = 11501  Accuracy = 0.8587948874010956\n",
      "idx = 11601  Accuracy = 0.8593224722006724\n",
      "idx = 11701  Accuracy = 0.859413725322622\n",
      "idx = 11801  Accuracy = 0.8595034319125497\n",
      "idx = 11901  Accuracy = 0.859003445088648\n",
      "idx = 12001  Accuracy = 0.8592617281893176\n",
      "idx = 12101  Accuracy = 0.859763655896207\n",
      "idx = 12201  Accuracy = 0.859847553479223\n",
      "idx = 12301  Accuracy = 0.8602552637996911\n",
      "idx = 12401  Accuracy = 0.8599306507539715\n",
      "idx = 12501  Accuracy = 0.8599312055035597\n",
      "idx = 12601  Accuracy = 0.8601698277914451\n",
      "idx = 12701  Accuracy = 0.8607196283757185\n",
      "idx = 12801  Accuracy = 0.8608702445121474\n",
      "idx = 12901  Accuracy = 0.8608634989535695\n",
      "idx = 13001  Accuracy = 0.8614721944465811\n",
      "idx = 13101  Accuracy = 0.861842607434547\n",
      "idx = 13201  Accuracy = 0.8619044011817286\n",
      "idx = 13301  Accuracy = 0.8619652657694911\n",
      "idx = 13401  Accuracy = 0.8623983284829491\n",
      "idx = 13501  Accuracy = 0.862676838752685\n",
      "idx = 13601  Accuracy = 0.863024777589883\n",
      "idx = 13701  Accuracy = 0.8636595868914678\n",
      "idx = 13801  Accuracy = 0.8638504456198827\n",
      "idx = 13901  Accuracy = 0.864254370189195\n",
      "idx = 14001  Accuracy = 0.8645811013499036\n",
      "idx = 14101  Accuracy = 0.8649741153109709\n",
      "idx = 14201  Accuracy = 0.8653615942539258\n",
      "idx = 14301  Accuracy = 0.8656737291098524\n",
      "idx = 14401  Accuracy = 0.8660509686827303\n",
      "idx = 14501  Accuracy = 0.8664919660713054\n",
      "idx = 14601  Accuracy = 0.867063899732895\n",
      "idx = 14701  Accuracy = 0.8674239847629412\n",
      "idx = 14801  Accuracy = 0.8678467671103304\n",
      "idx = 14901  Accuracy = 0.8681296557278035\n",
      "idx = 15001  Accuracy = 0.8682087860809279\n",
      "idx = 15101  Accuracy = 0.8685517515396332\n",
      "idx = 15201  Accuracy = 0.8688244194460891\n",
      "idx = 15301  Accuracy = 0.8688321024769623\n",
      "idx = 15401  Accuracy = 0.8693591325238621\n",
      "idx = 15501  Accuracy = 0.8694277788529772\n",
      "idx = 15601  Accuracy = 0.869880135888725\n",
      "idx = 15701  Accuracy = 0.8700719699382204\n",
      "idx = 15801  Accuracy = 0.8699449401936586\n",
      "idx = 15901  Accuracy = 0.8703226212187912\n",
      "idx = 16001  Accuracy = 0.8705080932441722\n",
      "idx = 16101  Accuracy = 0.8706912614123347\n",
      "idx = 16201  Accuracy = 0.8706869946299611\n",
      "idx = 16301  Accuracy = 0.8711122017054168\n",
      "idx = 16401  Accuracy = 0.8714712517529419\n",
      "idx = 16501  Accuracy = 0.8714623356160233\n",
      "idx = 16601  Accuracy = 0.8716944762363713\n",
      "idx = 16701  Accuracy = 0.8718639602419017\n",
      "idx = 16801  Accuracy = 0.8717933456341884\n",
      "idx = 16901  Accuracy = 0.8717235666528608\n",
      "idx = 17001  Accuracy = 0.8720075289688842\n",
      "idx = 17101  Accuracy = 0.8721127419449155\n",
      "idx = 17201  Accuracy = 0.8722167315853729\n",
      "idx = 17301  Accuracy = 0.8720883185943009\n",
      "idx = 17401  Accuracy = 0.8721337854146314\n",
      "idx = 17501  Accuracy = 0.8722930118278955\n",
      "idx = 17601  Accuracy = 0.8725072439065962\n",
      "idx = 17701  Accuracy = 0.87288853737077\n",
      "idx = 17801  Accuracy = 0.8729846637829335\n",
      "idx = 17901  Accuracy = 0.8732473046198537\n",
      "idx = 18001  Accuracy = 0.8732848175101383\n",
      "idx = 18101  Accuracy = 0.8735428981824208\n",
      "idx = 18201  Accuracy = 0.8734135487061151\n",
      "idx = 18301  Accuracy = 0.8736681055680018\n",
      "idx = 18401  Accuracy = 0.8738112059127221\n",
      "idx = 18501  Accuracy = 0.8737906059131939\n",
      "idx = 18601  Accuracy = 0.8740390301596689\n",
      "idx = 18701  Accuracy = 0.874124378375488\n",
      "idx = 18801  Accuracy = 0.8741556300196798\n",
      "idx = 18901  Accuracy = 0.8743981799904766\n",
      "idx = 19001  Accuracy = 0.8745329193200357\n",
      "idx = 19101  Accuracy = 0.8744568347206952\n",
      "idx = 19201  Accuracy = 0.8746940263527941\n",
      "idx = 19301  Accuracy = 0.8747733278068494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx = 19401  Accuracy = 0.8750579867017164\n",
      "idx = 19501  Accuracy = 0.8750833290600482\n",
      "idx = 19601  Accuracy = 0.8750573950308658\n",
      "idx = 19701  Accuracy = 0.8750317242779554\n",
      "idx = 19801  Accuracy = 0.8751578203121054\n",
      "idx = 19901  Accuracy = 0.8751821516506708\n",
      "idx = 20001  Accuracy = 0.8751562421878906\n",
      "idx = 20101  Accuracy = 0.8754788318989105\n",
      "idx = 20201  Accuracy = 0.8754517103113707\n",
      "idx = 20301  Accuracy = 0.8754741145756366\n",
      "idx = 20401  Accuracy = 0.8757413852262144\n",
      "idx = 20501  Accuracy = 0.8758109360518999\n",
      "idx = 20601  Accuracy = 0.8759768943255182\n",
      "idx = 20701  Accuracy = 0.875899714989614\n",
      "idx = 20801  Accuracy = 0.8761598000096149\n",
      "idx = 20901  Accuracy = 0.8763695516960911\n",
      "idx = 21001  Accuracy = 0.8765296890624256\n",
      "idx = 21101  Accuracy = 0.8766883086109664\n",
      "idx = 21201  Accuracy = 0.8768454318192538\n",
      "idx = 21301  Accuracy = 0.876249941317309\n",
      "idx = 21401  Accuracy = 0.876501098079529\n",
      "idx = 21501  Accuracy = 0.8767034091437608\n",
      "idx = 21601  Accuracy = 0.8768112587380213\n",
      "idx = 21701  Accuracy = 0.8769181143726096\n",
      "idx = 21801  Accuracy = 0.877115728636301\n",
      "idx = 21901  Accuracy = 0.8774028583169718\n",
      "idx = 22001  Accuracy = 0.8772328530521339\n",
      "idx = 22101  Accuracy = 0.8773358671553323\n",
      "idx = 22201  Accuracy = 0.8773929102292689\n",
      "idx = 22301  Accuracy = 0.8777633289986996\n",
      "idx = 22401  Accuracy = 0.8779072362840945\n",
      "idx = 22501  Accuracy = 0.8780054219812453\n",
      "idx = 22601  Accuracy = 0.878146984646697\n",
      "idx = 22701  Accuracy = 0.8783313510418044\n",
      "idx = 22801  Accuracy = 0.8783825270821455\n",
      "idx = 22901  Accuracy = 0.8786952534823806\n",
      "idx = 23001  Accuracy = 0.8785270205643233\n",
      "idx = 23101  Accuracy = 0.8787931258387083\n",
      "idx = 23201  Accuracy = 0.8788845308391879\n",
      "idx = 23301  Accuracy = 0.87893223466804\n",
      "idx = 23401  Accuracy = 0.8790649972223409\n",
      "idx = 23501  Accuracy = 0.8791115271690566\n",
      "idx = 23601  Accuracy = 0.8793695182407525\n",
      "idx = 23701  Accuracy = 0.8793299860765369\n",
      "idx = 23801  Accuracy = 0.8794588462669636\n",
      "idx = 23901  Accuracy = 0.8794611104137903\n",
      "idx = 24001  Accuracy = 0.8795466855547686\n",
      "idx = 24101  Accuracy = 0.879548566449525\n",
      "idx = 24201  Accuracy = 0.8795917524069253\n",
      "idx = 24301  Accuracy = 0.8797168840788445\n",
      "idx = 24401  Accuracy = 0.8798819720503258\n",
      "idx = 24501  Accuracy = 0.8798416391167707\n",
      "idx = 24601  Accuracy = 0.8800861753587252\n",
      "idx = 24701  Accuracy = 0.880207279057528\n",
      "idx = 24801  Accuracy = 0.8803274061529777\n",
      "idx = 24901  Accuracy = 0.880245773262118\n",
      "Accuracy on Test set: 0.88\n",
      "\n",
      "Examples of wrongs:\n",
      "4725 : This is, without a doubt, the most hilarious movie I've ever seen. Seriously, if the makers of this movie are ever discovered, they'll put guys like Jim Carrey out of a job. Rent \"Jack-O\" tonight! Believe me, you won't regret it!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-1ad1849987d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwrongs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrongs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Testing model...\")\n",
    "correct = 0\n",
    "wrongs = []\n",
    "for idx in range(len_of_test):\n",
    "    \n",
    "    if(idx%100 ==1):\n",
    "        print(\"idx =\",idx, \"/tAccuracy =\",correct/idx)\n",
    "    \n",
    "    with open(test_filenames[idx], \"r\") as f:\n",
    "        doc = nlp(f.read())\n",
    "    if(idx < len_of_test/2 and doc.cats[\"NEGATIVE\"] > doc.cats[\"POSITIVE\"] \n",
    "       or idx > len_of_test/2 and doc.cats[\"POSITIVE\"] > doc.cats[\"NEGATIVE\"]):\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrongs.append(idx)\n",
    "    \n",
    "print(\"Accuracy on Test set:\", correct/len_of_test)  # Accuracy = .88\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing examples of incorrect predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of wrongs:\n",
      "21787 : Incredibly hilarious mid-70's Italian Rootsploitation with lots of non-consensual S&M, lesbian sex, gratuitous racial cruelty etc...Few redeeming cinematic qualities, except for the fairly cool theme music with dubby \"African\" drums and flute. Brilliant sample dialog: <br /><br />White Slave Owner (to White Plantation Manager): \"You're so dumb, I'll bet you forgot to interrogate that n****r midwife!\"<br /><br />White Plantation Manager: \"Not only did I interrogate her, I did it so well she died before I could get any answers from her!\"<br /><br />All the black actors have 70's afros, and say \"yes, massa\" in a high-pitched voice. The female lead has sex with everybody on the plantation. 10 Stars for fans of tasteless sleaze.\n",
      "{'POSITIVE': 0.22175347805023193, 'NEGATIVE': 0.7782465219497681} \n",
      "\n",
      "23482 : People who say that some of what the Looney Tunes cartoons show is inappropriate for children have apparently forgotten something: they weren't originally intended for children. They were produced for the cinema, to be shown before feature films, and so they could show anything that they wanted. \"Tweety's S.O.S.\" has that bad puddy tat Sylvester finding Tweety aboard an ocean liner and boarding the ship to try and get him. But two things work against Sylvester: Granny is vehemently protecting the canary, and Sylvester easily gets seasick (we don't see him throwing up, but with his green face, they make it perfectly clear that he was doing just that!). And then of course, the seemingly cute Tweety has a bad-ass streak.<br /><br />It's just great to see how these cartoons weren't afraid to go all out; whatever they thought about showing, they showed. We need to understand that cartoons weren't always supposed to be cute entertainment for children. Really funny.\n",
      "{'POSITIVE': 0.3869238495826721, 'NEGATIVE': 0.6130761504173279} \n",
      "\n",
      "5637 : This odd little film starts out with the story of Bruno (Alex Linz) in a catholic school who has no friends and gets beat up everyday. He likes to wear dresses and his obese mother Angela who is a dressmaker doesn't think their is anything wrong with what her son likes. Angela complains to Mother Superior (Kathy Bates) but gets ignored and as the two of them walk back to they're car they are harassed by the other kids and are pelted with eggs. Bruno's father Dino (Gary Sinise) is divorced from Angela and is totally disgusted by his son being a sissy and practically disowns him. Bruno meets a new student at school named Shawniqua (Kiami Davael) who is a free spirit and dresses like Annie Oakley with cap pistols. Angela has a heart attack and Bruno's grandmother steps in to take care of him when Dino refuses.<br /><br />The film starts out with a very hard and unsympathetic look at all the characters involved. Angela has a great deal to do with Bruno wearing dresses as she practically encourages him. Dino was told when he was a young boy by his mother that he was a sissy because he liked opera and now he refuses to help Bruno when he needs it. The catholic school that Bruno attends is very unruly and all the kids run rampant and even call Shawniqua the \"N\" word. Once Shirley MacLaine steps in the film shifts and becomes more family oriented (So to speak). ****SPOILER ALERT**** The ending after the spelling bee is incredibly contrived and \"feel good\". Hugs and cheers for Bruno as reporters follow him and take his picture for their papers. All the while Shirley MacLaine is acting like the \"tough old broad\" who snaps at everyone. There is one thing about MacLaine's character in the film that no one has mention in these comments and it has to do with the masculine nature of her. I think the character of Helen might be a lesbian! She's very tough and strong and at one point in the film she shares a shot of whiskey with Bruno and smokes a cigar at the same time. I don't remember anyone in the film mentioning who her husband was or if she was ever married at all! This is why I think her character might be gay. Lots of other good actors appear in the film as well. Joey Lauren Adams, Jennifer Tilly, Brett Butler, Gwen Verdon and Lainie Kazan all should have taken a better look at the script before they signed on. I guess when they heard that MacLaine was directing that it would be an honor to be part of it. Very difficult to feel any remorse or understanding towards any of the characters and the subject matter is probably impossible for most to relate to. The actors are not bad but what exactly was MacLaine aiming for? Tolerance towards a young boy who wants to wear dresses and freedom of expression? We get that in the first 10 minutes, the rest of the time I was trying not to cringe.\n",
      "{'POSITIVE': 0.5629206895828247, 'NEGATIVE': 0.4370792806148529} \n",
      "\n",
      "6623 : It's not often I give two stars to a horror movie because horror is my favorite genre. A movie can be BAD in that it isn't a masterpiece but can be enjoyable on the basis of unintentional humour, bizarre characters, etc. A case in point are a great number of horror/sci-fiction movies from the 1940s to 1980s era. They are enjoyable for genre-buffs and guilty-pleasure seekers because their \"badness\" is entertaining. However, this movie has none of the humour or effective gory scenes of the \"Piranha\" (1978) original. <br /><br />I suppose in 1995 it was the heyday of political correctness so gore on TV was at a minimum. Now in the mid-2000s with the C.S.I. shows, TV's an absolute blood-fest! (Good for us horror fans!)<br /><br />William Katt and Alexandra Paul are no Bradford Dillman and Heather Menzies (the original 1978 stars.) It's not Katt's and Paul's faults but the writers and director who created this tepid turkey. How the main characters interact is the main flaw of this movie. I won't say how because that is part of the plot. <br /><br />This TV movie probably had a bigger budget than the original but flopped as good horror, as can be seen from the user votes here. Stick with the 1978 original if you're in the mood for a killer-fish movie!\n",
      "{'POSITIVE': 0.5165762901306152, 'NEGATIVE': 0.48342373967170715} \n",
      "\n",
      "5328 : this movie isn't that great...at all but it's good when you want to just laugh, because it's pretty ridiculous :) there are a lot of mistakes in it and it's cheesy. i got this movie for Christmas like 5 years ago but for some reason i've never given it away. i guess i just like it for a rainy day even though i only watch it like once a year. This is a very 90's movie so it's really funny to see how everyone dresses and acts. this movie is good for someone young...although come to think of it, i didn't even like it much when i was like 12 but that's just my personal opinion. the movie was really predictable. i wish it had had some extra weird twists but i guess it was trying to be an appropriate movie for everyone to enjoy. i think it was appropriate for the whole family but Hallie's dress was a bit unmodest but certainly appropriate enough for family material.\n",
      "{'POSITIVE': 0.9146126508712769, 'NEGATIVE': 0.08538728207349777} \n",
      "\n",
      "19033 : Like another poster mentioned Ch. 56 (a local Boston TV station) showed this multiple times over the years on Saturday afternoons. They paired it with the first sequel \"Return of the Ginat Majin\".<br /><br />Now I haven't seen it since then...but it never left me. Aside from the atrocious dubbing and faded color this was a pretty good fantasy. Technically it isn't horror...until the statue comes to life at the end. It's just about a village ruled over by an evil man. There's a giant stone statue there that the villagers keep praying to to help them...to no avail. But things go too far, the statute comes to life and destroys the bad guys...but then it starts going after the good guys too! Well-done with some cool special effects at the end (LOVED how he got rid of the main bad guy). Also there was an enchanted forest worked in which was kind of interesting too.<br /><br />No masterpiece but an unusual combo fantasy/horror film. Worth catching--but not if it's the dubbed print.\n",
      "{'POSITIVE': 0.4903183579444885, 'NEGATIVE': 0.5096816420555115} \n",
      "\n",
      "8056 : Maybe I was to young when I saw it. Perhaps I have not grown up with Grease and Elvis movies.<br /><br />I failed to get it. I get \"black\" comedy (Black Adder etc.). I get irony and spoofs. I don't get this one though.<br /><br />I made it a quest to find out the name of this movie (enlisting the help of people on usenet and the most excellent IMDb Message Boards) so it could be my first 1-pointer. Awful!\n",
      "{'POSITIVE': 0.87832111120224, 'NEGATIVE': 0.1216789036989212} \n",
      "\n",
      "10204 : This movie is very scary with scenes where the Devil uses Gabriels horn to open Heaven and pull the good angel-dogs out and imprision them on Alkatraz. The devil sings and dances to a few songs about the joys of being bad, and at one point, eats a live rat.<br /><br />We got this movie free with a pizza. You get what you pay for.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POSITIVE': 0.8238601088523865, 'NEGATIVE': 0.17613986134529114} \n",
      "\n",
      "18669 : What about DJ Cash Money??? This film fails in part by not covering the mid to late 80s. There was only a small mention of DJ Cheese in 86.<br /><br />Also, it's Grandmixer \"DST\", not \"DXT\"!!!!!\n",
      "{'POSITIVE': 0.06487777084112167, 'NEGATIVE': 0.9351223111152649} \n",
      "\n",
      "22331 : Remember that friend in college who always insisted you rent the weirdest movie possible? This is the movie he would have made if he'd had the chance.<br /><br />I wish I could tell you exactly what Sea of Dust was about. It pretends to be the story of a doctor who gets sucked into weird goings on in the \"Black Forest.\" He goes there to help, but ends up being caught between two young women, both of whom he seems to have a thing for. But that's just scratching the surface. This is the kind of movie where things just randomly happen...and not nice thing. People are constantly being whipped and stabbed. There's a pair of creepy little girls who appear to have walked out of The Shining. Tom Savini is some kind of imaginary religious figure who decides he doesn't want to be imaginary anymore. He's got a plan to take over the world by sharing Jesus suffering.<br /><br />On some level, this is a movie about sex. It's one without nudity, which was a disappointment, but there's no mistaking the intent. On another whole level, it's a stoner's paradise. Unexpected stuff happens so often that it stops being unexpected. By the time the doctor travels through his girlfriend's birth canal to be reborn, you'll just chalk it up to the crazy nature of the flick.<br /><br />On the down side, the film is pretty wordy. Some of the points are hammered home over and over. If you're watching it with a bunch of stoned friends, this might prove an asset.\n",
      "{'POSITIVE': 0.19300180673599243, 'NEGATIVE': 0.8069981932640076} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExamples of wrongs:\")\n",
    "random.shuffle(wrongs)\n",
    "for i in range(10):\n",
    "    with open(test_filenames[wrongs[i]],\"r\") as f:\n",
    "        temp = f.read()\n",
    "        print(wrongs[i], \":\", temp)\n",
    "        print(nlp(temp).cats,\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_vectors(vectors):\n",
    "    with open(os.path.join(os.getcwd(), \"/Users/Ben/Desktop/Vital Strategies/trained models/nlpVectors/Asgn2vectors.tsv\"), \"w\") as f:\n",
    "        for vector in vectors:\n",
    "            for feature in vector:\n",
    "                f.write(str(feature))\n",
    "                f.write(\"\\t\")\n",
    "                f.write(\"1.0\\t1.0\\t1.0\\t\") # BS so I can display on embedding projector\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    f.close()\n",
    "def save_metadata(titles,labels):\n",
    "    with open(os.path.join(os.getcwd(), \"/Users/Ben/Desktop/Vital Strategies/trained models/nlpVectors/Asgn2metadata.tsv\"), \"w\") as f:\n",
    "        f.write(\"title\\tcategory\\n\")\n",
    "        for i, title in enumerate(titles):\n",
    "            f.write(str(title))\n",
    "            f.write(\"\\t\")\n",
    "            f.write(labels[i])\n",
    "            f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8e20843f9f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msave_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msave_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-2e239d31ffef>\u001b[0m in \u001b[0;36msave_metadata\u001b[0;34m(titles, labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\\tcategory\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not int"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES_TO_SAVE = 1000\n",
    "\n",
    "idxs = list(range(len_of_test))\n",
    "random.shuffle(idxs)\n",
    "idxs = idxs[:NUM_EXAMPLES_TO_SAVE]\n",
    "\n",
    "vectors = [] #vectors in 1D - How POSTIVE it thinks a title is\n",
    "titles = []\n",
    "labels = []\n",
    "\n",
    "for idx in idxs:\n",
    "    with open(test_filenames[idx], \"r\") as f:\n",
    "        temp = f.read()\n",
    "        doc = nlp(temp)\n",
    "        vectors.append([doc.cats[\"POSITIVE\"]])\n",
    "        titles.append(idx)\n",
    "        if(idx < len_of_test/2 and doc.cats[\"NEGATIVE\"] > doc.cats[\"POSITIVE\"] ):\n",
    "            labels.append(\"NEGATIVE\")\n",
    "        elif (idx > len_of_test/2 and doc.cats[\"POSITIVE\"] > doc.cats[\"NEGATIVE\"]):\n",
    "            labels.append(\"POSTIVE\")\n",
    "        else:\n",
    "            labels.append(\"INCORRECT\")\n",
    "save_vectors(vectors)\n",
    "save_metadata(titles,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printatidx(idx):\n",
    "    print(\"idx:\",idx)\n",
    "    with open(test_filenames[idx], \"r\") as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 14857\n",
      "This movie was well done. It covers the difficulties a returning Vietnam veteran has in dealing with the horrors of war. Unfortunately the writers chose to focus on a Vet who had been involved in an act of atrocity. I was in Vietnam and only once heard of such an act by one who witnessed it. The offender was prosecuted and sentenced to many years in Leavenworth.<br /><br />The notion that only vets involved in atrocities had emotional problems is a disservice to all who served. All of the soldiers I knew personally or knew of by word of mouth were honorable soldiers who respected even the enemy and believed they were there to halt the spread of Communism. The biggest problem was coming home to learn that many Americans were opposed to the war. That is what caused many Veterans to feel they had taken part in something less than honorable. Not the manner in which they served.<br /><br />The ending depicted the father acting more as a belligerent bully than a loving, caring father. For that I gave it a 7 out of 10. Had the ending allowed for a degree of acceptance I could have rated it a 9.<br /><br />Most decent men will come home from war with guilt and emotional scars. They need acceptance and understanding to overcome that. I pray that the public is more understanding of our present day Veterans than it was in the the Vietnam era.\n"
     ]
    }
   ],
   "source": [
    "printatidx(14857)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
